hp@krusovice:~/Work/marcipan $ time cat  /big1/Work/HunPos/Web2/Raw/web2.1.owol.tnttagged | head -20000000 | ./test 10000000
token limit = 10000000
1. collection started
 2. run started
  3. collecting terminfos started
  3. collecting terminfos finished: 185.760000 doing 10000027 tokens; speed = 53833.048019 tokens/sec
  4. flushing started
  4. flushing finished: 17.280000 doing 10000027 tokens; speed = 578705.266204 tokens/sec
 2. run finished: 203.040000 doing 10000027 tokens; speed = 49251.512017 tokens/sec
 5. run started
  6. collecting terminfos started
  6. collecting terminfos finished: 103.430000 doing 8920658 tokens; speed = 86248.264527 tokens/sec
  7. flushing started
  7. flushing finished: 20.910000 doing 8920658 tokens; speed = 426621.616451 tokens/sec
 5. run finished: 124.340000 doing 8920658 tokens; speed = 71744.072704 tokens/sec
 8. merging started
 8. merging finished: 53.840000
1. collection finished: 381.220000 doing 1076565 documents; speed = 2823.999266 documents/sec

Nagy gaz van. Kisebb memoriaval gyorsabb

time cat  /big1/Work/HunPos/Web2/Raw/web2.1.owol.tnttagged | head -20000000 | ./test 5000000
token limit = 5000000
1. collection started
 2. run started
  3. collecting terminfos started
  3. collecting terminfos finished: 53.460000 doing 5000033 tokens; speed = 93528.488590 tokens/sec
  4. flushing started
  4. flushing finished: 10.620000 doing 5000033 tokens; speed = 470812.900188 tokens/sec
 2. run finished: 64.080000 doing 5000033 tokens; speed = 78027.980649 tokens/sec
 5. run started
  6. collecting terminfos started
  6. collecting terminfos finished: 54.430000 doing 5000026 tokens; speed = 91861.583685 tokens/sec
  7. flushing started
  7. flushing finished: 10.650000 doing 5000026 tokens; speed = 469486.009390 tokens/sec
 5. run finished: 65.080000 doing 5000026 tokens; speed = 76828.918254 tokens/sec
 8. run started
  9. collecting terminfos started
  9. collecting terminfos finished: 34.630000 doing 5000001 tokens; speed = 144383.511406 tokens/sec
  10. flushing started
  10. flushing finished: 7.950000 doing 5000001 tokens; speed = 628930.943396 tokens/sec
 8. run finished: 42.580000 doing 5000001 tokens; speed = 117426.045092 tokens/sec
 11. run started
  12. collecting terminfos started
  12. collecting terminfos finished: 30.940000 doing 3920625 tokens; speed = 126717.032967 tokens/sec
  13. flushing started
  13. flushing finished: 7.690000 doing 3920625 tokens; speed = 509834.200260 tokens/sec
 11. run finished: 38.630000 doing 3920625 tokens; speed = 101491.716283 tokens/sec
 14. merging started
 14. merging finished: 41.880000
1. collection finished: 252.250000 doing 1076565 documents; speed = 4267.849356 documents/sec

real    4m12.271s
user    4m6.271s
sys     0m8.808s


EBBOL IS LATSZIK A BAJ

p@krusovice:~/Work/marcipan $ time cat  /big1/Work/HunPos/Web2/Raw/web2.1.owol.tnttagged | head -2000000 | ./test 21000000
token limit = 21000000
1. collection started
 2. run started
  3. collecting terminfos started
  3. collecting terminfos finished: 12.200000 doing 1889519 tokens; speed = 154878.606557 tokens/sec
  4. writing terminfos started
  4. writing terminfos finished: 3.630000 doing 1889519 tokens; speed = 520528.650138 tokens/sec
 2. run finished: 15.830000 doing 1889519 tokens; speed = 119363.171194 tokens/sec
1. collection finished: 15.830000 doing 110208 documents; speed = 6961.970941 documents/sec

real    0m15.863s
user    0m15.577s
sys     0m0.599s
hp@krusovice:~/Work/marcipan $ time cat  /big1/Work/HunPos/Web2/Raw/web2.1.owol.tnttagged | head -4000000 | ./test 41000000
token limit = 41000000
1. collection started
 2. run started
  3. collecting terminfos started
  3. collecting terminfos finished: 38.390000 doing 3785967 tokens; speed = 98618.572545 tokens/sec
  4. writing terminfos started
  4. writing terminfos finished: 6.040000 doing 3785967 tokens; speed = 626815.728477 tokens/sec
 2. run finished: 44.430000 doing 3785967 tokens; speed = 85211.951384 tokens/sec
1. collection finished: 44.430000 doing 213512 documents; speed = 4805.581814 documents/sec

real    0m44.491s
user    0m43.942s
sys     0m1.132s

KETSZER AKKORA KORPUSZNAL CSOKKEN A SEBESSEG

ezek profileja

head kisebb.prof 
Flat profile:

Each sample counts as 0.01 seconds.
  %   cumulative   self              self     total           
 time   seconds   seconds    calls   s/call   s/call  name    
 43.52     65.79    65.79 12354305     0.00     0.00  caml_fl_allocate
 17.22     91.82    26.03     3290     0.01     0.01  mark_slice
  3.72     97.44     5.62  1889519     0.00     0.00  camlMfhash__update_165
  2.94    101.88     4.44     2115     0.00     0.00  sweep_slice
  2.73    106.00     4.12  4933491     0.00     0.00  camlBlockList__add_84
hp@krusovice:~/Work/marcipan $ head nagyobb.prof 
Flat profile:

Each sample counts as 0.01 seconds.
  %   cumulative   self              self     total           
 time   seconds   seconds    calls   s/call   s/call  name    
 61.26    263.62   263.62 23307171     0.00     0.00  caml_fl_allocate
 12.27    316.44    52.82     6396     0.01     0.01  mark_slice
  2.63    327.77    11.33  3785967     0.00     0.00  camlMfhash__update_165
  1.94    336.10     8.33  9804156     0.00     0.00  camlBlockList__add_84
  1.71    343.45     7.35     4199     0.00     0.00  sweep_slice


na a nagyobbik exp novekedos blocklisttel

1. collection started
 2. run started
  3. collecting terminfos started
  3. collecting terminfos finished: 48.000000 doing 3785967 tokens; speed = 78874.312500 tokens/sec
  4. writing terminfos started
  4. writing terminfos finished: 10.980000 doing 3785967 tokens; speed = 344805.737705 tokens/sec
 2. run finished: 58.980000 doing 3785967 tokens; speed = 64190.691760 tokens/sec
1. collection finished: 58.980000 doing 213512 documents; speed = 3620.074602 documents/sec

real    0m59.014s
user    0m58.679s
sys     0m0.837s
hp@krusovice:~/Work/marcipan $ gprof test | head
Flat profile:

Each sample counts as 0.01 seconds.
  %   cumulative   self              self     total           
 time   seconds   seconds    calls   s/call   s/call  name    
 58.50    166.11   166.11 10259267     0.00     0.00  caml_fl_allocate
  7.29    186.82    20.71     5584     0.00     0.00  mark_slice
  4.09    198.44    11.63  3785967     0.00     0.00  camlMfhash__update_165
  3.08    207.18     8.74  9804156     0.00     0.00  camlBlockList__add_84
  2.31    213.73     6.55  3680204     0.00     0.00  compare_val
hp@krusovice:~/Work/marcipan $


javitva a memoria:
export OCAMLRUNPARAM="b,i=640000,s=640000"

p@krusovice:~/Work/marcipan $ time cat  /big1/Work/HunPos/Web2/Raw/web2.1.owol.tnttagged | head -4000000 | ./test 41000000
token limit = 41000000
1. collection started
 2. run started
  3. collecting terminfos started
  3. collecting terminfos finished: 12.720000 doing 3785967 tokens; speed = 297638.915094 tokens/sec
  4. writing terminfos started
  4. writing terminfos finished: 2.070000 doing 3785967 tokens; speed = 1828969.565217 tokens/sec
 2. run finished: 14.790000 doing 3785967 tokens; speed = 255981.541582 tokens/sec
1. collection finished: 14.790000 doing 213512 documents; speed = 14436.240703 documents/sec

real    0m14.832s
user    0m14.423s
sys     0m0.902s
hp@krusovice:~/Work/marcipan $ time cat  /big1/Work/HunPos/Web2/Raw/web2.1.owol.tnttagged | head -2000000 | ./test 41000000
token limit = 41000000
1. collection started
 2. run started
  3. collecting terminfos started
  3. collecting terminfos finished: 6.070000 doing 1889519 tokens; speed = 311288.138386 tokens/sec
  4. writing terminfos started
  4. writing terminfos finished: 1.310000 doing 1889519 tokens; speed = 1442380.916031 tokens/sec
 2. run finished: 7.380000 doing 1889519 tokens; speed = 256032.384824 tokens/sec
1. collection finished: 7.380000 doing 110208 documents; speed = 14933.333333 documents/sec

real    0m7.411s
user    0m7.195s
sys     0m0.479s
hp@krusovice:~/Work/marcipan $ time cat  /big1/Work/HunPos/Web2/Raw/web2.1.owol.tnttagged | head -4000000 | ./test 2000000
token limit = 2000000
1. collection started
 2. run started
  3. collecting terminfos started
  3. collecting terminfos finished: 6.470000 doing 2000016 tokens; speed = 309121.483771 tokens/sec
  4. flushing started
  4. flushing finished: 1.470000 doing 2000016 tokens; speed = 1360555.102041 tokens/sec
 2. run finished: 7.940000 doing 2000016 tokens; speed = 251891.183879 tokens/sec
 5. run started
  6. collecting terminfos started
  6. collecting terminfos finished: 6.070000 doing 1785951 tokens; speed = 294225.864909 tokens/sec
  7. flushing started
  7. flushing finished: 1.650000 doing 1785951 tokens; speed = 1082394.545455 tokens/sec
 5. run finished: 7.720000 doing 1785951 tokens; speed = 231340.803109 tokens/sec
 8. merging started
 8. merging finished: 3.400000
1. collection finished: 19.060000 doing 213512 documents; speed = 11202.098636 documents/sec

real    0m19.078s
user    0m17.908s
sys     0m1.713s


konstanst 16 novekedessel is fasza
hp@krusovice:~/Work/marcipan $ time cat  /big1/Work/HunPos/Web2/Raw/web2.1.owol.tnttagged | head -4000000 | ./test 41000000
token limit = 41000000
1. collection started
 2. run started
  3. collecting terminfos started
  3. collecting terminfos finished: 11.180000 doing 3785967 tokens; speed = 338637.477639 tokens/sec
  4. writing terminfos started
  4. writing terminfos finished: 2.380000 doing 3785967 tokens; speed = 1590742.436975 tokens/sec
 2. run finished: 13.560000 doing 3785967 tokens; speed = 279201.106195 tokens/sec
1. collection finished: 13.560000 doing 213512 documents; speed = 15745.722714 documents/sec

real    0m13.598s
user    0m13.366s
sys     0m0.768s
hp@krusovice:~/Work/marcipan $ time cat  /big1/Work/HunPos/Web2/Raw/web2.1.owol.tnttagged | head -2000000 | ./test 41000000
token limit = 41000000
1. collection started
 2. run started
  3. collecting terminfos started
  3. collecting terminfos finished: 5.240000 doing 1889519 tokens; speed = 360595.229008 tokens/sec
  4. writing terminfos started
  4. writing terminfos finished: 1.580000 doing 1889519 tokens; speed = 1195898.101266 tokens/sec
 2. run finished: 6.820000 doing 1889519 tokens; speed = 277055.571848 tokens/sec
1. collection finished: 6.820000 doing 110208 documents; speed = 16159.530792 documents/sec

real    0m6.839s
user    0m6.704s
sys     0m0.390s
hp@krusovice:~/Work/marcipan $ time cat  /big1/Work/HunPos/Web2/Raw/web2.1.owol.tnttagged | head -4000000 | ./test 2000000
token limit = 2000000
1. collection started
 2. run started
  3. collecting terminfos started
  3. collecting terminfos finished: 5.560000 doing 2000016 tokens; speed = 359715.107914 tokens/sec
  4. flushing started
  4. flushing finished: 1.570000 doing 2000016 tokens; speed = 1273895.541401 tokens/sec
 2. run finished: 7.130000 doing 2000016 tokens; speed = 280507.152875 tokens/sec
 5. run started
  6. collecting terminfos started
  6. collecting terminfos finished: 4.870000 doing 1785951 tokens; speed = 366725.051335 tokens/sec
  7. flushing started
  7. flushing finished: 1.860000 doing 1785951 tokens; speed = 960188.709677 tokens/sec
 5. run finished: 6.730000 doing 1785951 tokens; speed = 265371.619614 tokens/sec
 8. merging started
 8. merging finished: 3.350000
1. collection finished: 17.210000 doing 213512 documents; speed = 12406.275421 documents/sec


a teljes
1. collection finished: 340.130000 doing 4332330 documents; speed = 12737.276924 documents/sec


A hosszas fejlesztgetes utan igy lelassultunk. Ez most constans 32 blok merettel van.

time cat  /big1/Work/HunPos/Web2/Raw/web2.1.owol.tnttagged | head -4000000 | ./test qindex build 2000000
token limit = 2000000
1. collection started
 2. run started
  3. collecting terminfos started
  3. collecting terminfos finished: 7.790000 doing 2000016 tokens; speed = 256741.463415 tokens/sec
  4. flushing started
  4. flushing finished: 2.030000 doing 2000016 tokens; speed = 985229.556650 tokens/sec
 2. run finished: 9.820000 doing 2000016 tokens; speed = 203667.617108 tokens/sec
 5. run started
  6. collecting terminfos started
  6. collecting terminfos finished: 6.300000 doing 1785951 tokens; speed = 283484.285714 tokens/sec
  7. flushing started
  7. flushing finished: 1.730000 doing 1785951 tokens; speed = 1032341.618497 tokens/sec
 5. run finished: 8.030000 doing 1785951 tokens; speed = 222409.838107 tokens/sec
 8. merging started
 8. merging finished: 4.620000
1. collection finished: 22.470000 doing 213512 documents; speed = 9502.091678 documents/sec

16-os block merettel

cat  /big1/Work/HunPos/Web2/Raw/web2.1.owol.tnttagged | head -4000000 | ./test qindex build 2000000
token limit = 2000000
1. collection started
 2. run started
  3. collecting terminfos started
  3. collecting terminfos finished: 7.810000 doing 2000016 tokens; speed = 256083.994878 tokens/sec
  4. flushing started
  4. flushing finished: 2.010000 doing 2000016 tokens; speed = 995032.835821 tokens/sec
 2. run finished: 9.820000 doing 2000016 tokens; speed = 203667.617108 tokens/sec
 5. run started
  6. collecting terminfos started
  6. collecting terminfos finished: 6.890000 doing 1785951 tokens; speed = 259209.143687 tokens/sec
  7. flushing started
  7. flushing finished: 1.570000 doing 1785951 tokens; speed = 1137548.407643 tokens/sec
 5. run finished: 8.460000 doing 1785951 tokens; speed = 211105.319149 tokens/sec
 8. merging started
 8. merging finished: 4.970000
1. collection finished: 23.250000 doing 213512 documents; speed = 9183.311828 documents/sec

Ugyanez egy menetben gyorsabb kicsit.

token limit = 20000000
1. collection started
 2. run started
  3. collecting terminfos started
  3. collecting terminfos finished: 14.750000 doing 3785967 tokens; speed = 256675.728814 tokens/sec
  4. writing terminfos started
  4. writing terminfos finished: 4.410000 doing 3785967 tokens; speed = 858495.918367 tokens/sec
 2. run finished: 19.160000 doing 3785967 tokens; speed = 197597.442589 tokens/sec
1. collection finished: 19.160000 doing 213512 documents; speed = 11143.632568 documents/sec

3416192 2006-10-01 11:58 docindex

feltehetoleg innen a hiba:
Value too large for defined data type [lstat(...)]
This is a limitation in the OCaml interface to the Unix system calls. (The problem is that the OCaml library uses 32-bit integers to represent file positions. The maximal positive 'int' in OCaml is about 2.1E9. We hope that the OCaml team will someday provide an alternative interface that uses 64-bit integers.
http://www.cis.upenn.edu/~bcpierce/unison/download/stable/unison-2.9.1/unison-manual.html#rshmeth

megoldva a kerdes. Int64 a pozico, es tomoritve taroljuk. A DocMeta fajl lett 1 G-nel nagyon

3400681 2006-10-01 12:00 qindex/docindex


na most meg nem lehet betolteni a lexikont. szamoljunk kicsit
7 millio token van

mennyi memoriat fogyasztunk?
s = string overhed
7.179.624 * s
7.179.624 * 12 (atlag string hossz) 86.283.080
7179624 * 16 (pointer szamok)
7179624 * 4 (tuple)
7179624 * k (Con overhed)
2000000 * k (Con Overhead, mert sok feje van)

(20 + 12 + k +s ) 7.0 

lexiconhoz epitunk egy indexet
visszaigazoljuk 40911
politikáját 32726
kéri 24548
gyár 16367
belépőt 8185



2007-07-08 
Lucene keresés

IndexSearcher csinálja a munkát. Searcher az apa, az hozza létre a search hívására a Hits-et, ami a hívótól, itt most IndexSearcher kéri le elsőször a 100 legjobb dokumentumot. A Hits-et kapja vissza az alkalmazás, abban lehet menni a találatok között. Ha valaki a 101-et kéri, akkor újra keresünk.
Ezt még majd meg kell nézni miért jó.

A Hits getMoreDocs()-ja hívódik, ha nincs elég dokumentum a Hits-ben. Ez lekér n darad találatot és beteszi egy tömbbe. A találatok relevanciáját normálja, hogy az elsőé 1-nél ne legyen nagyobb.

Hát szerintem semmi durva optimalizációt nem csinál. A Query át tudja magát alakítani Weight-é, amihez tartozik egy scorer. Például a TermScorer csinálja, ami a tfidf világban a dolga. A Similiraty meg a Scorer alapján számolja a query-doc hasonlóságot. Ha jól látom, akkor a TermDocs-ot olvasva tényleg végigmegy mindenen.

TermScorer. Egy term súlyát adja meg a dokumentumokban. Megkapja a TermDocs-ot, Similarity-t. Végigmegy a dokumentumokon, kiolvassa 32-esevél a dokumentumokat és a tf értéket. Van egy cache-e scoreCache, amiben a valódi súlyokat cachelei 1..32 közötti tf értékekhez. Mert  sqrt(tf) értéket számol a defaultSimilarity és azt még szorozza a Term query-ben felvett súlyával. Megspórol egy gyökvonást és egy szorzást, ha a dokban a term 1..32-szor fordul elo.

A BooleanScorer2 a DisjunctionScrorer meg ConjuctionScorer-t használja. Utobbi az AND-nek felel meg. Egy láncolt listában tárolja a Scorereket úgy, hogy mindig előrébb legyen, aminek kisebb a köv docidje. Ne feledd egy Scorer is tudja a next-et, skipTo-t, mint a termDocs. Ezután 

while (more && first().doc() < last().doc()) { // find doc w/ all clauses
  more = first().skipTo(last().doc());      // skip first upto last
  scorers.addLast(scorers.removeFirst());   // move first to last
}

azaz minden Scorer-t addig mozgat, amíg ugyanarra a dokra nem állnak.

Érdekes a ReqExclScorer, ami egy exclude es requried keresés keveréke. Értelemszerűen a next()-ben van az exclusion logika. Addig megy a req scrorer, amíg el nem ér egy az exclude.next által visszadott doc-ot. Azt átugorja.


. Next-re addig megy


A memoria beallitasokra igen erzekeny a kicsike
 3. collecting terminfos finished: 48.830766 doing 12618531 tokens; speed = 258413.537891 tokens/sec
  4. writing terminfos started
  4. writing terminfos finished: 14.419260 doing 12618531 tokens; speed = 875116.406806 tokens/sec
 2. run finished: 63.250191 doing 12618531 tokens; speed = 199501.863955 tokens/sec
1. collection finished: 63.253209 doing 49530 documents; speed = 783.043276 documents/sec


De 1 perc alatt leindexeli a magyar hirlapot!!
Most osszemerjuk a lemurral. Igaz, nem teljesen tudjuk, hogy az milyen indexet epit, nem tudtunk belenezni

Lemur
Macintosh-2:~/work/lemur_test hp$ time BuildIndex build_param mh/mh2002*

real    1m15.140s
user    1m6.910s
sys     0m6.144s

Nagyon ugy nez ki, hogy megverjuk a lemur project-et. Kicsivel, de szep az ocamltol es tolem

Macintosh-2:~/work/marcipan hp$ time cat ../lemur_test/mh/mh2002*.xml | iconv -f UTF-8 -t ISO-8859-2 -c  | ./main.native mhindex indexmh 100000000
token limit = 100000000
1. collection started
 2. run started
  3. collecting terminfos started
  3. collecting terminfos finished: 47.530842 doing 12618531 tokens; speed = 265480.906061 tokens/sec
  4. writing terminfos started
  4. writing terminfos finished: 14.394679 doing 12618531 tokens; speed = 876610.794864 tokens/sec
 2. run finished: 61.925738 doing 12618531 tokens; speed = 203768.762514 tokens/sec
1. collection finished: 61.928842 doing 49530 documents; speed = 799.788893 documents/sec

real    1m3.114s
user    1m3.134s
sys     0m4.877s

Lemur keresésről. Először is van az Index.hpp, ami az index interfésze. Ebből most nekünk a  
virtual DocInfoList *docInfoList(TERMID_T termID) const=0;
fgv érdekes, ami egy term dokumentumait adja meg. DocInfo docid, termCount-ből áll. Az InvDocList 
(kis következetlenség, hogy nem InvDocInfoList ) az implementáció, amit használunk indexeléskor is. 
Egy tömbben tárolja a docid, frekvencia értékeket, ha elfogy a hely, akkor duplázza a tömböt. Binárisan
tömörítve írja ki, beolvasáskor kitömörít. Csak iterálni tudunk rajta, nincs skipTo, mint a 
lucene-ben.

RetrievalMethod scoreCollection metodusa kapja a query-t és (int,double) vectort (IndexedRealVector) ad vissza. TextRetrievalMethod alosztály kezele a TextQueryket, amit string, double párok listája. Hihetetlen ez a C++. Ennek még egy alosztálya a ArrayQueryRep, ami egy tömbben tárolja a termeket. Ez most nem érdekes, nálunk azért ez csak egy sima lista lesz.

TextRetrievalMethod a következőt csinálja: van egy nagy tömbje, ScoreAccumulator, ebbe gyűjti majd a dokumentumok scoreját. Ez igazából két tömb: egy int status (be van-e állítva a score) és maga a double score. Namármost a TextQueryRetMethod:scoreInvertedIndex két lépében számol. Először

minden termre a queryben, keresd ki a docInfoList-et
menj végig minden dokumentumot és
scoreFunc()->matchedTermWeight -t hivd meg és a dokumentum akkumulátorához add hozzá a mostani term súlyát.

ezután menj végig az összes dokumentumon újra, és ha volt hozzá score, akkor
scoreFunc()->adjustedScore -t számolj, ami megkapja a teljes querzt, a docot és az eddigi súlyt.

De ket dolog van. DocModel és a scoreFunc, valahogy e kettőben van elrejtve a valódi ranking. Van a SimpleKLDocModel aminek gyereke a JelinekMercerDocModel. Előbbiben nem teljesen értem a termWeight metódust
/// term weighting function, weight(w) = p_seen(w)/p_unseen(w)

Pedig a JelinekMercerDocModel pont azt számolja, amit vártunk, persze tud interpolálni és backoffolni is. Na jó, én többet nem nézem ezt a C++ kódot. A SimpleKLRetMethodban van egy SimpleKLScoreFunc osztály. Az csinálja a számolást.

Oke, engem már csak a docLength kiolvasás érdekel, aztán ennél áramvonalasabb megoldás jön.Ezta SimpleKLRetMethod kéri le az indextől. InvIndex dtlookup -jánál kell folytatni. Abban van benne a doc hossza.

csákesz

Elmentünk sörözni. Most jöttünk vissza. Ez a dtlookup egy dokumentumszámnyi hosszú tömb, amibe betölti a docIdhez a hosszt, a termjeire mutató pointert stb. Semmi különös nincs benne. Persze nem értem, hogy mi van, ha nem fér a memóriába a docNum x legalább 8 byte. Na mindegy. Szóval ez nálunk a forwardIndex volt. Itt InvFPTermList -eket írunk ki minden dokhoz, ami a termjei gyakorisággal, szóval bag of words. Ezeknek van egy index fájlja: ez megy a dtlookup-ba. A trükk az, hogy amíg nem kell a dokumentum termjeihez nyúlni, addig a hossz a memóriában lesz. A hossz mellett ott van még, hogy melyik fájl, melyik pozicióján kezdődik a doc forwardIndexe (gondolom itt figyelnek arra, hogy egy fájl nem lehet nagyobb egy bizonyos méretnél)

Ja, itt van az InvPushIndex.cpp-ben: 
 if (offset+(3*sizeof(lemur::api::LOC_T))+(tls*sizeof(LocatedTerm)) > maxfile) {
  writetlist.close();
  std::stringstream nameStr;
  nameStr << name << DTINDEX << dtfiles.size();
  string docfname = nameStr.str();
  dtfiles.push_back(docfname);
  writetlist.open(docfname.c_str(), ios::binary | ios::out);
  offset = 0;
}







